{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:33:12.839208Z",
     "iopub.status.busy": "2021-04-02T01:33:12.838556Z",
     "iopub.status.idle": "2021-04-02T01:33:19.176971Z",
     "shell.execute_reply": "2021-04-02T01:33:19.176327Z"
    },
    "id": "U8l4RJ0XRPEm"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7577e6f7fe9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# You'll generate plots of attention in order to see which parts of an image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# our model focuses on during captioning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\context.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunction_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrewriter_config_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\core\\framework\\function_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0m_b\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdescriptor\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_descriptor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmessage\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_message\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreflection\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_reflection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\img\\lib\\site-packages\\google\\protobuf\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pkg_resources'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeclare_namespace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0m__path__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pkgutil'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__path__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\img\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   3254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3256\u001b[1;33m \u001b[1;33m@\u001b[0m\u001b[0m_call_aside\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3257\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_initialize_master_working_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3258\u001b[0m     \"\"\"\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\img\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36m_call_aside\u001b[1;34m(f, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3238\u001b[0m \u001b[1;31m# from jaraco.functools 1.3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3239\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_call_aside\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3240\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3241\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\img\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36m_initialize_master_working_set\u001b[1;34m()\u001b[0m\n\u001b[0;32m   3267\u001b[0m     \u001b[0mat\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mown\u001b[0m \u001b[0mrisk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3268\u001b[0m     \"\"\"\n\u001b[1;32m-> 3269\u001b[1;33m     \u001b[0mworking_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWorkingSet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_master\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3270\u001b[0m     \u001b[0m_declare_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworking_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworking_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\img\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36m_build_master\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[0mPrepare\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmaster\u001b[0m \u001b[0mworking\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \"\"\"\n\u001b[1;32m--> 573\u001b[1;33m         \u001b[0mws\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    574\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0m__main__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__requires__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\img\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, entries)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_entry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\img\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36madd_entry\u001b[1;34m(self, entry)\u001b[0m\n\u001b[0;32m    620\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentry_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 622\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfind_distributions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    623\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\img\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36mfind_on_path\u001b[1;34m(importer, path_item, only)\u001b[0m\n\u001b[0;32m   2072\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpath_item_entries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2073\u001b[0m         \u001b[0mfullpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_item\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2074\u001b[1;33m         \u001b[0mfactory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_item\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monly\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2075\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfactory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2076\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\img\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36mdist_factory\u001b[1;34m(path_item, entry, only)\u001b[0m\n\u001b[0;32m   2083\u001b[0m     is_dist_info = (\n\u001b[0;32m   2084\u001b[0m         \u001b[0mlower\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.dist-info'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2085\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_item\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2086\u001b[0m     )\n\u001b[0;32m   2087\u001b[0m     \u001b[0mis_meta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_egg_info\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mis_dist_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6qbGw8MRPE5"
   },
   "source": [
    "## Download and prepare the MS-COCO dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:33:19.185062Z",
     "iopub.status.busy": "2021-04-02T01:33:19.183806Z",
     "iopub.status.idle": "2021-04-02T01:47:36.145754Z",
     "shell.execute_reply": "2021-04-02T01:47:36.146255Z"
    },
    "id": "krQuPYTtRPE7"
   },
   "outputs": [],
   "source": [
    "# Download caption annotation files\n",
    "annotation_folder = '/annotations/'\n",
    "if not os.path.exists(os.path.abspath('.') + annotation_folder):\n",
    "  annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
    "                                           cache_subdir=os.path.abspath('.'),\n",
    "                                           origin='http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
    "                                           extract=True)\n",
    "  annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n",
    "  os.remove(annotation_zip)\n",
    "else:\n",
    "    annotation_file ='annotations/captions_train2014.json'\n",
    "# Download image files\n",
    "image_folder = '/train2014/'\n",
    "if not os.path.exists(os.path.abspath('.') + image_folder):\n",
    "  image_zip = tf.keras.utils.get_file('train2014.zip',\n",
    "                                      cache_subdir=os.path.abspath('.'),\n",
    "                                      origin='http://images.cocodataset.org/zips/train2014.zip',\n",
    "                                      extract=True)\n",
    "  PATH = os.path.dirname(image_zip) + image_folder\n",
    "  os.remove(image_zip)\n",
    "else:\n",
    "  PATH = os.path.abspath('.') + image_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aANEzb5WwSzg"
   },
   "source": [
    "##limit the size of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:47:36.170038Z",
     "iopub.status.busy": "2021-04-02T01:47:36.169322Z",
     "iopub.status.idle": "2021-04-02T01:47:37.009729Z",
     "shell.execute_reply": "2021-04-02T01:47:37.010165Z"
    },
    "id": "m8iBJCyVB2ud"
   },
   "outputs": [],
   "source": [
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:47:37.059890Z",
     "iopub.status.busy": "2021-04-02T01:47:37.034008Z",
     "iopub.status.idle": "2021-04-02T01:47:37.582238Z",
     "shell.execute_reply": "2021-04-02T01:47:37.582728Z"
    },
    "id": "miER7EHMB3Ge"
   },
   "outputs": [],
   "source": [
    "# Group all captions together having the same image ID.\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "for val in annotations['annotations']:\n",
    "  caption = f\"<start> {val['caption']} <end>\"\n",
    "  image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n",
    "  image_path_to_caption[image_path].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:47:37.622100Z",
     "iopub.status.busy": "2021-04-02T01:47:37.606601Z",
     "iopub.status.idle": "2021-04-02T01:47:37.651315Z",
     "shell.execute_reply": "2021-04-02T01:47:37.651730Z"
    },
    "id": "7vvqkqYGMhvm"
   },
   "outputs": [],
   "source": [
    "image_paths = list(image_path_to_caption.keys())\n",
    "random.shuffle(image_paths)\n",
    "\n",
    "# Select the first 6000 image_paths from the shuffled set.\n",
    "# Approximately each image id has 5 captions associated with it, so that will\n",
    "# lead to 30,000 examples.\n",
    "train_image_paths = image_paths[:6000]\n",
    "print(len(train_image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:47:37.664987Z",
     "iopub.status.busy": "2021-04-02T01:47:37.664245Z",
     "iopub.status.idle": "2021-04-02T01:47:37.666686Z",
     "shell.execute_reply": "2021-04-02T01:47:37.666051Z"
    },
    "id": "hrmdtMX8Lnyh"
   },
   "outputs": [],
   "source": [
    "train_captions = []\n",
    "img_name_vector = []\n",
    "\n",
    "for image_path in train_image_paths:\n",
    "  caption_list = image_path_to_caption[image_path]\n",
    "  train_captions.extend(caption_list)\n",
    "  img_name_vector.extend([image_path] * len(caption_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:47:37.670957Z",
     "iopub.status.busy": "2021-04-02T01:47:37.670273Z",
     "iopub.status.idle": "2021-04-02T01:47:37.875011Z",
     "shell.execute_reply": "2021-04-02T01:47:37.875456Z"
    },
    "id": "RhCND0bCUP11"
   },
   "outputs": [],
   "source": [
    "print(train_captions[0])\n",
    "Image.open(img_name_vector[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cSW4u-ORPFQ"
   },
   "source": [
    "## Preprocess the images using InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:47:37.881346Z",
     "iopub.status.busy": "2021-04-02T01:47:37.880690Z",
     "iopub.status.idle": "2021-04-02T01:47:37.883142Z",
     "shell.execute_reply": "2021-04-02T01:47:37.882626Z"
    },
    "id": "zXR0217aRPFR"
   },
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDvIu4sXRPFV"
   },
   "source": [
    "## Initialize InceptionV3 and load the pretrained Imagenet weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:47:37.890684Z",
     "iopub.status.busy": "2021-04-02T01:47:37.890004Z",
     "iopub.status.idle": "2021-04-02T01:47:46.361763Z",
     "shell.execute_reply": "2021-04-02T01:47:46.361100Z"
    },
    "id": "RD3vW4SsRPFW"
   },
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rERqlR3WRPGO"
   },
   "source": [
    "## Caching the features extracted from InceptionV3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:47:46.395377Z",
     "iopub.status.busy": "2021-04-02T01:47:46.394329Z",
     "iopub.status.idle": "2021-04-02T01:48:37.678678Z",
     "shell.execute_reply": "2021-04-02T01:48:37.677957Z"
    },
    "id": "Dx_fvbVgRPGQ"
   },
   "outputs": [],
   "source": [
    "# Get unique images\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "# Feel free to change batch_size according to your system configuration\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(\n",
    "  load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n",
    "\n",
    "for img, path in image_dataset:\n",
    "  batch_features = image_features_extract_model(img)\n",
    "  batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    "  for bf, p in zip(batch_features, path):\n",
    "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "    np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"saved/encoded_train_features.pkl\",\"wb\") as f:\n",
    " #   pickle.dump(encoding_train,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyqH3zFwRPFi"
   },
   "source": [
    "## Preprocess and tokenize the captions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:48:37.683898Z",
     "iopub.status.busy": "2021-04-02T01:48:37.683239Z",
     "iopub.status.idle": "2021-04-02T01:48:37.685457Z",
     "shell.execute_reply": "2021-04-02T01:48:37.684894Z"
    },
    "id": "HZfK8RhQRPFj"
   },
   "outputs": [],
   "source": [
    "# Find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:48:37.745517Z",
     "iopub.status.busy": "2021-04-02T01:48:37.724452Z",
     "iopub.status.idle": "2021-04-02T01:48:38.113730Z",
     "shell.execute_reply": "2021-04-02T01:48:38.113174Z"
    },
    "id": "oJGE34aiRPFo"
   },
   "outputs": [],
   "source": [
    "# Choose the top 5000 words from the vocabulary\n",
    "top_k = 5000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:48:38.118300Z",
     "iopub.status.busy": "2021-04-02T01:48:38.117632Z",
     "iopub.status.idle": "2021-04-02T01:48:38.119732Z",
     "shell.execute_reply": "2021-04-02T01:48:38.119249Z"
    },
    "id": "8Q44tNQVRPFt"
   },
   "outputs": [],
   "source": [
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:48:38.163985Z",
     "iopub.status.busy": "2021-04-02T01:48:38.153601Z",
     "iopub.status.idle": "2021-04-02T01:48:38.453197Z",
     "shell.execute_reply": "2021-04-02T01:48:38.452627Z"
    },
    "id": "0fpJb5ojRPFv"
   },
   "outputs": [],
   "source": [
    "# Create the tokenized vectors\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:48:38.462916Z",
     "iopub.status.busy": "2021-04-02T01:48:38.462260Z",
     "iopub.status.idle": "2021-04-02T01:48:38.589404Z",
     "shell.execute_reply": "2021-04-02T01:48:38.588806Z"
    },
    "id": "AidglIZVRPF4"
   },
   "outputs": [],
   "source": [
    "# Pad each vector to the max_length of the captions\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:48:38.596564Z",
     "iopub.status.busy": "2021-04-02T01:48:38.595818Z",
     "iopub.status.idle": "2021-04-02T01:48:38.597664Z",
     "shell.execute_reply": "2021-04-02T01:48:38.598074Z"
    },
    "id": "gL0wkttkRPGA"
   },
   "outputs": [],
   "source": [
    "# Calculates the max_length, which is used to store the attention weights\n",
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3CD75nDpvTI"
   },
   "source": [
    "## Split the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:48:38.633916Z",
     "iopub.status.busy": "2021-04-02T01:48:38.633166Z",
     "iopub.status.idle": "2021-04-02T01:48:38.635412Z",
     "shell.execute_reply": "2021-04-02T01:48:38.634854Z"
    },
    "id": "iS7DDMszRPGF"
   },
   "outputs": [],
   "source": [
    "img_to_cap_vector = collections.defaultdict(list)\n",
    "for img, cap in zip(img_name_vector, cap_vector):\n",
    "  img_to_cap_vector[img].append(cap)\n",
    "\n",
    "# Create training and validation sets using an 80-20 split randomly.\n",
    "img_keys = list(img_to_cap_vector.keys())\n",
    "random.shuffle(img_keys)\n",
    "\n",
    "slice_index = int(len(img_keys)*0.8)\n",
    "img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
    "\n",
    "img_name_train = []\n",
    "cap_train = []\n",
    "for imgt in img_name_train_keys:\n",
    "  capt_len = len(img_to_cap_vector[imgt])\n",
    "  img_name_train.extend([imgt] * capt_len)\n",
    "  cap_train.extend(img_to_cap_vector[imgt])\n",
    "\n",
    "img_name_val = []\n",
    "cap_val = []\n",
    "for imgv in img_name_val_keys:\n",
    "  capv_len = len(img_to_cap_vector[imgv])\n",
    "  img_name_val.extend([imgv] * capv_len)\n",
    "  cap_val.extend(img_to_cap_vector[imgv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:48:38.640084Z",
     "iopub.status.busy": "2021-04-02T01:48:38.639416Z",
     "iopub.status.idle": "2021-04-02T01:48:38.642140Z",
     "shell.execute_reply": "2021-04-02T01:48:38.642528Z"
    },
    "id": "XmViPkRFRPGH"
   },
   "outputs": [],
   "source": [
    "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEWM9xrYcg45"
   },
   "source": [
    "## Create a tf.data dataset for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:48:38.647502Z",
     "iopub.status.busy": "2021-04-02T01:48:38.646894Z",
     "iopub.status.idle": "2021-04-02T01:48:38.649116Z",
     "shell.execute_reply": "2021-04-02T01:48:38.648634Z"
    },
    "id": "Q3TnZ1ToRPGV"
   },
   "outputs": [],
   "source": [
    "# Feel free to change these parameters according to your system's configuration\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = top_k + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:48:38.653444Z",
     "iopub.status.busy": "2021-04-02T01:48:38.652625Z",
     "iopub.status.idle": "2021-04-02T01:48:38.654847Z",
     "shell.execute_reply": "2021-04-02T01:48:38.654366Z"
    },
    "id": "SmZS2N0bXG3T"
   },
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def map_func(img_name, cap):\n",
    "  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "  return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:48:38.714136Z",
     "iopub.status.busy": "2021-04-02T01:48:38.688372Z",
     "iopub.status.idle": "2021-04-02T01:48:39.474421Z",
     "shell.execute_reply": "2021-04-02T01:48:39.474815Z"
    },
    "id": "FDF_Nm3tRPGZ"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrvoDphgRPGd"
   },
   "source": [
    "## Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:48:39.482877Z",
     "iopub.status.busy": "2021-04-02T01:48:39.482179Z",
     "iopub.status.idle": "2021-04-02T01:48:39.484417Z",
     "shell.execute_reply": "2021-04-02T01:48:39.483930Z"
    },
    "id": "ja2LFTMSdeV3"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, features, hidden):\n",
    "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "    # hidden shape == (batch_size, hidden_size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "    # attention_hidden_layer shape == (batch_size, 64, units)\n",
    "    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                         self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # score shape == (batch_size, 64, 1)\n",
    "    # This gives you an unnormalized score for each image feature.\n",
    "    score = self.V(attention_hidden_layer)\n",
    "\n",
    "    # attention_weights shape == (batch_size, 64, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * features\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:48:39.489979Z",
     "iopub.status.busy": "2021-04-02T01:48:39.489049Z",
     "iopub.status.idle": "2021-04-02T01:48:39.491771Z",
     "shell.execute_reply": "2021-04-02T01:48:39.491282Z"
    },
    "id": "AZ7R1RxHRPGf"
   },
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:48:39.501125Z",
     "iopub.status.busy": "2021-04-02T01:48:39.500117Z",
     "iopub.status.idle": "2021-04-02T01:48:39.502832Z",
     "shell.execute_reply": "2021-04-02T01:48:39.502357Z"
    },
    "id": "V9UbGQmERPGi"
   },
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "  def __init__(self, embedding_dim, units, vocab_size):\n",
    "    super(RNN_Decoder, self).__init__()\n",
    "    self.units = units\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "  def call(self, x, features, hidden):\n",
    "    # defining attention as a separate model\n",
    "    context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # shape == (batch_size, max_length, hidden_size)\n",
    "    x = self.fc1(output)\n",
    "\n",
    "    # x shape == (batch_size * max_length, hidden_size)\n",
    "    x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size * max_length, vocab)\n",
    "    x = self.fc2(x)\n",
    "\n",
    "    return x, state, attention_weights\n",
    "\n",
    "  def reset_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:48:39.509337Z",
     "iopub.status.busy": "2021-04-02T01:48:39.508301Z",
     "iopub.status.idle": "2021-04-02T01:48:39.537063Z",
     "shell.execute_reply": "2021-04-02T01:48:39.536531Z"
    },
    "id": "Qs_Sr03wRPGk"
   },
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:48:39.543584Z",
     "iopub.status.busy": "2021-04-02T01:48:39.542597Z",
     "iopub.status.idle": "2021-04-02T01:48:39.545172Z",
     "shell.execute_reply": "2021-04-02T01:48:39.544727Z"
    },
    "id": "-bYN7xA0RPGl"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6A3Ni64joyab"
   },
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:48:39.553957Z",
     "iopub.status.busy": "2021-04-02T01:48:39.552992Z",
     "iopub.status.idle": "2021-04-02T01:48:39.555163Z",
     "shell.execute_reply": "2021-04-02T01:48:39.555549Z"
    },
    "id": "PpJAqPMWo0uE"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:48:39.560183Z",
     "iopub.status.busy": "2021-04-02T01:48:39.559277Z",
     "iopub.status.idle": "2021-04-02T01:48:39.561984Z",
     "shell.execute_reply": "2021-04-02T01:48:39.561485Z"
    },
    "id": "fUkbqhc_uObw"
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "  # restoring the latest checkpoint in checkpoint_path\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHod7t72RPGn"
   },
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:48:39.566197Z",
     "iopub.status.busy": "2021-04-02T01:48:39.565231Z",
     "iopub.status.idle": "2021-04-02T01:48:39.567763Z",
     "shell.execute_reply": "2021-04-02T01:48:39.567232Z"
    },
    "id": "Vt4WZ5mhJE-E"
   },
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:48:39.575388Z",
     "iopub.status.busy": "2021-04-02T01:48:39.574440Z",
     "iopub.status.idle": "2021-04-02T01:48:39.576980Z",
     "shell.execute_reply": "2021-04-02T01:48:39.576396Z"
    },
    "id": "sqgyz2ANKlpU"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "  loss = 0\n",
    "\n",
    "  # initializing the hidden state for each batch\n",
    "  # because the captions are not related from image to image\n",
    "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "      features = encoder(img_tensor)\n",
    "\n",
    "      for i in range(1, target.shape[1]):\n",
    "          # passing the features through the decoder\n",
    "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "          loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "          # using teacher forcing\n",
    "          dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "  total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "  return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T01:48:39.583840Z",
     "iopub.status.busy": "2021-04-02T01:48:39.582898Z",
     "iopub.status.idle": "2021-04-02T02:05:08.368215Z",
     "shell.execute_reply": "2021-04-02T02:05:08.368655Z"
    },
    "id": "UlA4VIQpRPGo"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "      ckpt_manager.save()\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:05:08.388459Z",
     "iopub.status.busy": "2021-04-02T02:05:08.387197Z",
     "iopub.status.idle": "2021-04-02T02:05:08.531959Z",
     "shell.execute_reply": "2021-04-02T02:05:08.531457Z"
    },
    "id": "1Wm83G-ZBPcC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGvOcLQKghXN"
   },
   "source": [
    "## Captionioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:05:08.540198Z",
     "iopub.status.busy": "2021-04-02T02:05:08.539560Z",
     "iopub.status.idle": "2021-04-02T02:05:08.541377Z",
     "shell.execute_reply": "2021-04-02T02:05:08.541803Z"
    },
    "id": "RCWpDtyNRPGs"
   },
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n",
    "                                                 -1,\n",
    "                                                 img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input,\n",
    "                                                         features,\n",
    "                                                         hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:05:08.547848Z",
     "iopub.status.busy": "2021-04-02T02:05:08.547263Z",
     "iopub.status.idle": "2021-04-02T02:05:08.549193Z",
     "shell.execute_reply": "2021-04-02T02:05:08.549567Z"
    },
    "id": "fD_y7PD6RPGt"
   },
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for i in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[i], (8, 8))\n",
    "        grid_size = max(np.ceil(len_result/2), 2)\n",
    "        ax = fig.add_subplot(grid_size, grid_size, i+1)\n",
    "        ax.set_title(result[i])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:05:08.555410Z",
     "iopub.status.busy": "2021-04-02T02:05:08.554801Z",
     "iopub.status.idle": "2021-04-02T02:05:10.500706Z",
     "shell.execute_reply": "2021-04-02T02:05:10.501131Z"
    },
    "id": "7x8RiPHe_4qI"
   },
   "outputs": [],
   "source": [
    "# captions on the validation set\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "real_caption = ' '.join([tokenizer.index_word[i]\n",
    "                        for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "print('Real Caption:', real_caption)\n",
    "print('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(image, result, attention_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "model2 = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "tokenizer2 = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:05:10.506458Z",
     "iopub.status.busy": "2021-04-02T02:05:10.505880Z",
     "iopub.status.idle": "2021-04-02T02:05:12.629940Z",
     "shell.execute_reply": "2021-04-02T02:05:12.630379Z"
    },
    "id": "9Psd1quzaAWg"
   },
   "outputs": [],
   "source": [
    "#image_url = 'https://tensorflow.org/images/surf.jpg'\n",
    "#image_url=\"dog.jpg\"\n",
    "\n",
    "from tkinter import *\n",
    "from tkinter import filedialog\n",
    "from PIL import Image\n",
    "root=Tk()\n",
    "top=Toplevel()\n",
    "image_url = filedialog.askopenfilename(initialdir = \"/\",\n",
    "                                          title = \"Select a File\",\n",
    "                                          filetypes = ((\"Image files\",\n",
    "                                                        \"*.jpg*\"),\n",
    "                                                       (\"all files\",\n",
    "                                                        \"*.*\")))\n",
    "top.mainloop()\n",
    "#image_extension = image_url[-4:]\n",
    "\n",
    "#image_path = tf.keras.utils.get_file('image'+image_extension, origin=image_url)\n",
    "image_path=image_url\n",
    "result, attention_plot = evaluate(image_path)\n",
    "print('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(image_path, result, attention_plot)\n",
    "# opening the image\n",
    "Image.open(image_path)\n",
    "cation=' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(cation,inpu):\n",
    "    keyt = str(\"y\")\n",
    "    while(keyt==str(\"y\")):\n",
    "        answer_text = cation\n",
    "        question = inpu\n",
    "        # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "        input_ids = tokenizer2.encode(question, answer_text)\n",
    "\n",
    "        print('The input has a total of {:} tokens.'.format(len(input_ids)))\n",
    "        # BERT only needs the token IDs, but for the purpose of inspecting the\n",
    "        # tokenizer's behavior, let's also get the token strings and display them.\n",
    "        tokens = tokenizer2.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "        # For each token and its id...\n",
    "        for token, id in zip(tokens, input_ids):\n",
    "\n",
    "            # If this is the [SEP] token, add some space around it to make it stand out.\n",
    "            if id == tokenizer2.sep_token_id:\n",
    "                print('')\n",
    "\n",
    "            # Print the token string and its ID in two columns.\n",
    "            print('{:<12} {:>6,}'.format(token, id))\n",
    "\n",
    "            if id == tokenizer2.sep_token_id:\n",
    "                print('')\n",
    "\n",
    "        # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "        sep_index = input_ids.index(tokenizer2.sep_token_id)\n",
    "\n",
    "        # The number of segment A tokens includes the [SEP] token istelf.\n",
    "        num_seg_a = sep_index + 1\n",
    "\n",
    "        # The remainder are segment B.\n",
    "        num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "        # Construct the list of 0s and 1s.\n",
    "        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "        # There should be a segment_id for every input token.\n",
    "        assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "        # Run our example through the model.\n",
    "        start_scores, end_scores = model2(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                         token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n",
    "        # Find the tokens with the highest `start` and `end` scores.\n",
    "        answer_start = torch.argmax(start_scores)\n",
    "        answer_end = torch.argmax(end_scores)\n",
    "\n",
    "        # Combine the tokens in the answer and print it out.\n",
    "        answer = ' '.join(tokens[answer_start:answer_end+1])\n",
    "\n",
    "        #print('Answer: \"' + answer + '\"')\n",
    "        # Start with the first token.\n",
    "        answer = tokens[answer_start]\n",
    "\n",
    "        # Select the remaining answer tokens and join them with whitespace.\n",
    "        for i in range(answer_start + 1, answer_end + 1):\n",
    "\n",
    "            # If it's a subword token, then recombine it with the previous token.\n",
    "            if tokens[i][0:2] == '##':\n",
    "                answer += tokens[i][2:]\n",
    "\n",
    "            # Otherwise, add a space then the token.\n",
    "            else:\n",
    "                answer += ' ' + tokens[i]\n",
    "        print(question)\n",
    "        print('Answer: \"' + answer + '\"')\n",
    "        return 'Answer: \"' + answer + '\"'\n",
    "        #keyt=str(input(\"Do you want to ask another question?\")).lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from tkinter import *\n",
    "\n",
    "# pip install pillow\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "class Window(Frame):\n",
    "    def __init__(self, master=None):\n",
    "        Frame.__init__(self, master)\n",
    "        self.master = master\n",
    "        self.pack(fill=BOTH, expand=1)\n",
    "        \n",
    "        load = Image.open(image_url)\n",
    "       \n",
    "        load=load.resize((1000, 600), Image.ANTIALIAS)\n",
    "        render = ImageTk.PhotoImage(load)\n",
    "        img = Label(self, image=render)\n",
    "        img.image = render\n",
    "        img.pack(side=TOP,padx=50)\n",
    "        \n",
    "\n",
    "root = Tk()\n",
    "\n",
    "app = Window(root)\n",
    "captiom=\"Caption: \"+str(cation)\n",
    "root.wm_title(\"ASKImage-CaptionBOT\")\n",
    "root.geometry(\"1280x915\")\n",
    "label1 = Label(root, text=captiom)\n",
    "label1.config(font=('helvetica', 18))\n",
    "label1.pack(side=TOP,padx=50)\n",
    "\n",
    "entry1 = Entry (root,width=100) \n",
    "entry1.pack(side=BOTTOM,pady=50,padx=100)\n",
    "entry1.config(font=('helvetica', 18))\n",
    "label2 = Label(root, text='Ask a Question:')\n",
    "label2.config(font=('helvetica', 22))\n",
    "label2.pack(side=BOTTOM,padx=120)\n",
    "def ct(cation):\n",
    "    L=ask(cation,entry1.get())\n",
    "    label3 = Label(root,text=L)\n",
    "    label3.pack(side=TOP,padx=10)\n",
    "    label3.config(font=('helvetica', 16))\n",
    "\n",
    "button1 = Button(text='Ask', bg='brown', fg='white',command=partial(ct,captiom), font=('helvetica', 18, 'bold'))\n",
    "button1.pack(side=RIGHT,padx=10,pady=0)\n",
    "\n",
    "\n",
    "root.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "image_captioning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
